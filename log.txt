hduser@VBox:~$ hdfs namenode -format
2018-01-18 16:40:28,028 INFO namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = VBox/127.0.1.1
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 3.0.0
STARTUP_MSG:   classpath = /home/hduser/hadoop/etc/hadoop:/home/hduser/hadoop/share/hadoop/common/lib/kerby-config-1.0.1.jar:/home/hduser/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/hduser/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/home/hduser/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/home/hduser/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/hduser/hadoop/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/home/hduser/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/hduser/hadoop/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/home/hduser/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/home/hduser/hadoop/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/home/hduser/hadoop/share/hadoop/common/lib/jersey-json-1.19.jar:/home/hduser/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/hduser/hadoop/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/home/hduser/hadoop/share/hadoop/common/lib/jsch-0.1.54.jar:/home/hduser/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/home/hduser/hadoop/share/hadoop/common/lib/netty-3.10.5.Final.jar:/home/hduser/hadoop/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/home/hduser/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/hduser/hadoop/share/hadoop/common/lib/asm-5.0.4.jar:/home/hduser/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/home/hduser/hadoop/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/home/hduser/hadoop/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/home/hduser/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/home/hduser/hadoop/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/home/hduser/hadoop/share/hadoop/common/lib/token-provider-1.0.1.jar:/home/hduser/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/home/hduser/hadoop/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/home/hduser/hadoop/share/hadoop/common/lib/re2j-1.1.jar:/home/hduser/hadoop/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/home/hduser/hadoop/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/home/hduser/hadoop/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/home/hduser/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/hduser/hadoop/share/hadoop/common/lib/snappy-java-1.0.5.jar:/home/hduser/hadoop/share/hadoop/common/lib/kerb-util-1.0.1.jar:/home/hduser/hadoop/share/hadoop/common/lib/curator-client-2.12.0.jar:/home/hduser/hadoop/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/home/hduser/hadoop/share/hadoop/common/lib/accessors-smart-1.2.jar:/home/hduser/hadoop/share/hadoop/common/lib/avro-1.7.7.jar:/home/hduser/hadoop/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/home/hduser/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/hduser/hadoop/share/hadoop/common/lib/commons-lang3-3.4.jar:/home/hduser/hadoop/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/home/hduser/hadoop/share/hadoop/common/lib/jersey-core-1.19.jar:/home/hduser/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/hduser/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/home/hduser/hadoop/share/hadoop/common/lib/json-smart-2.3.jar:/home/hduser/hadoop/share/hadoop/common/lib/junit-4.11.jar:/home/hduser/hadoop/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/home/hduser/hadoop/share/hadoop/common/lib/kerby-util-1.0.1.jar:/home/hduser/hadoop/share/hadoop/common/lib/jackson-core-2.7.8.jar:/home/hduser/hadoop/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/home/hduser/hadoop/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/home/hduser/hadoop/share/hadoop/common/lib/jersey-server-1.19.jar:/home/hduser/hadoop/share/hadoop/common/lib/zookeeper-3.4.9.jar:/home/hduser/hadoop/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/home/hduser/hadoop/share/hadoop/common/lib/xz-1.0.jar:/home/hduser/hadoop/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/home/hduser/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/hduser/hadoop/share/hadoop/common/lib/stax2-api-3.1.4.jar:/home/hduser/hadoop/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/home/hduser/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/hduser/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/home/hduser/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/hduser/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/home/hduser/hadoop/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/home/hduser/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/hduser/hadoop/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/home/hduser/hadoop/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/hduser/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/home/hduser/hadoop/share/hadoop/common/lib/curator-framework-2.12.0.jar:/home/hduser/hadoop/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/hduser/hadoop/share/hadoop/common/lib/kerb-common-1.0.1.jar:/home/hduser/hadoop/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/home/hduser/hadoop/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/home/hduser/hadoop/share/hadoop/common/lib/kerb-client-1.0.1.jar:/home/hduser/hadoop/share/hadoop/common/lib/kerb-server-1.0.1.jar:/home/hduser/hadoop/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/home/hduser/hadoop/share/hadoop/common/lib/kerb-core-1.0.1.jar:/home/hduser/hadoop/share/hadoop/common/lib/jersey-servlet-1.19.jar:/home/hduser/hadoop/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/home/hduser/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/hduser/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/hduser/hadoop/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/home/hduser/hadoop/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/home/hduser/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/hduser/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/home/hduser/hadoop/share/hadoop/common/lib/metrics-core-3.0.1.jar:/home/hduser/hadoop/share/hadoop/common/hadoop-kms-3.0.0.jar:/home/hduser/hadoop/share/hadoop/common/hadoop-nfs-3.0.0.jar:/home/hduser/hadoop/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/home/hduser/hadoop/share/hadoop/common/hadoop-common-3.0.0.jar:/home/hduser/hadoop/share/hadoop/hdfs:/home/hduser/hadoop/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/commons-net-3.1.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/gson-2.2.4.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/asm-5.0.4.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/jettison-1.1.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/re2j-1.1.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/avro-1.7.7.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/json-smart-2.3.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/xz-1.0.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/hduser/hadoop/share/hadoop/hdfs/lib/paranamer-2.3.jar:/home/hduser/hadoop/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/home/hduser/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/home/hduser/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/home/hduser/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/home/hduser/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/home/hduser/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/home/hduser/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/home/hduser/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/home/hduser/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/home/hduser/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/home/hduser/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/home/hduser/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/home/hduser/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/home/hduser/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/home/hduser/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/home/hduser/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/home/hduser/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/home/hduser/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/home/hduser/hadoop/share/hadoop/yarn:/home/hduser/hadoop/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/guice-4.0.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/joni-2.1.2.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/jersey-client-1.19.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/commons-el-1.0.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/commons-csv-1.0.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/home/hduser/hadoop/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/home/hduser/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/home/hduser/hadoop/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/home/hduser/hadoop/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/home/hduser/hadoop/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/home/hduser/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/home/hduser/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/home/hduser/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/home/hduser/hadoop/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/home/hduser/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/home/hduser/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/home/hduser/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/home/hduser/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/home/hduser/hadoop/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/home/hduser/hadoop/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/home/hduser/hadoop/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/home/hduser/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/home/hduser/hadoop/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/home/hduser/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z
STARTUP_MSG:   java = 1.8.0_151
************************************************************/
2018-01-18 16:40:28,044 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2018-01-18 16:40:28,061 INFO namenode.NameNode: createNameNode [-format]
2018-01-18 16:40:28,435 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Formatting using clusterid: CID-507a137f-1081-4cb1-9f76-cd0388f21568
2018-01-18 16:40:29,131 INFO namenode.FSEditLog: Edit logging is async:true
2018-01-18 16:40:29,161 INFO namenode.FSNamesystem: KeyProvider: null
2018-01-18 16:40:29,164 INFO namenode.FSNamesystem: fsLock is fair: true
2018-01-18 16:40:29,169 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2018-01-18 16:40:29,180 INFO namenode.FSNamesystem: fsOwner             = hduser (auth:SIMPLE)
2018-01-18 16:40:29,180 INFO namenode.FSNamesystem: supergroup          = supergroup
2018-01-18 16:40:29,180 INFO namenode.FSNamesystem: isPermissionEnabled = true
2018-01-18 16:40:29,180 INFO namenode.FSNamesystem: HA Enabled: false
2018-01-18 16:40:29,272 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2018-01-18 16:40:29,305 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2018-01-18 16:40:29,306 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2018-01-18 16:40:29,316 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2018-01-18 16:40:29,317 INFO blockmanagement.BlockManager: The block deletion will start around 2018 Jan 18 16:40:29
2018-01-18 16:40:29,323 INFO util.GSet: Computing capacity for map BlocksMap
2018-01-18 16:40:29,324 INFO util.GSet: VM type       = 64-bit
2018-01-18 16:40:29,326 INFO util.GSet: 2.0% max memory 1.7 GB = 35.5 MB
2018-01-18 16:40:29,327 INFO util.GSet: capacity      = 2^22 = 4194304 entries
2018-01-18 16:40:29,373 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false
2018-01-18 16:40:29,383 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2018-01-18 16:40:29,383 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2018-01-18 16:40:29,383 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
2018-01-18 16:40:29,383 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
2018-01-18 16:40:29,384 INFO blockmanagement.BlockManager: defaultReplication         = 1
2018-01-18 16:40:29,384 INFO blockmanagement.BlockManager: maxReplication             = 512
2018-01-18 16:40:29,384 INFO blockmanagement.BlockManager: minReplication             = 1
2018-01-18 16:40:29,384 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
2018-01-18 16:40:29,384 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms
2018-01-18 16:40:29,384 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
2018-01-18 16:40:29,384 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2018-01-18 16:40:29,472 INFO util.GSet: Computing capacity for map INodeMap
2018-01-18 16:40:29,472 INFO util.GSet: VM type       = 64-bit
2018-01-18 16:40:29,472 INFO util.GSet: 1.0% max memory 1.7 GB = 17.7 MB
2018-01-18 16:40:29,472 INFO util.GSet: capacity      = 2^21 = 2097152 entries
2018-01-18 16:40:29,474 INFO namenode.FSDirectory: ACLs enabled? false
2018-01-18 16:40:29,474 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true
2018-01-18 16:40:29,474 INFO namenode.FSDirectory: XAttrs enabled? true
2018-01-18 16:40:29,475 INFO namenode.NameNode: Caching file names occurring more than 10 times
2018-01-18 16:40:29,486 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true
2018-01-18 16:40:29,495 INFO util.GSet: Computing capacity for map cachedBlocks
2018-01-18 16:40:29,495 INFO util.GSet: VM type       = 64-bit
2018-01-18 16:40:29,495 INFO util.GSet: 0.25% max memory 1.7 GB = 4.4 MB
2018-01-18 16:40:29,496 INFO util.GSet: capacity      = 2^19 = 524288 entries
2018-01-18 16:40:29,513 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2018-01-18 16:40:29,513 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2018-01-18 16:40:29,513 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2018-01-18 16:40:29,522 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
2018-01-18 16:40:29,522 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2018-01-18 16:40:29,526 INFO util.GSet: Computing capacity for map NameNodeRetryCache
2018-01-18 16:40:29,526 INFO util.GSet: VM type       = 64-bit
2018-01-18 16:40:29,527 INFO util.GSet: 0.029999999329447746% max memory 1.7 GB = 545.1 KB
2018-01-18 16:40:29,527 INFO util.GSet: capacity      = 2^16 = 65536 entries
2018-01-18 16:40:29,589 INFO namenode.FSImage: Allocated new BlockPoolId: BP-839627358-127.0.1.1-1516286429566
2018-01-18 16:40:29,652 INFO common.Storage: Storage directory /tmp/hadoop-hduser/dfs/name has been successfully formatted.
2018-01-18 16:40:29,709 INFO namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-hduser/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression
2018-01-18 16:40:30,013 INFO namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-hduser/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 391 bytes saved in 0 seconds.
2018-01-18 16:40:30,050 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
2018-01-18 16:40:30,062 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at VBox/127.0.1.1
************************************************************/
hduser@VBox:~$ start-all.sh
WARNING: Attempting to start all Apache Hadoop daemons as hduser in 10 seconds.
WARNING: This is not a recommended production deployment configuration.
WARNING: Use CTRL-C to abort.
Starting namenodes on [localhost]
Starting datanodes
Starting secondary namenodes [VBox]
2018-01-18 16:41:03,839 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Starting resourcemanager
Starting nodemanagers
hduser@VBox:~$ /usr/lib/jvm/java-8-openjdk-amd64/bin/jps
6480 NodeManager
5863 DataNode
6104 SecondaryNameNode
6859 Jps
6350 ResourceManager
5727 NameNode
hduser@VBox:~$ hadoop fs -ls
2018-01-18 16:44:31,518 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
ls: `.': No such file or directory
hduser@VBox:~$ hadoop fs -ls /
2018-01-18 16:44:42,184 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
hduser@VBox:~$ hadoop fs -mkdir /datafiles
2018-01-18 16:45:17,597 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
hduser@VBox:~$ hadoop fs -ls /
2018-01-18 16:45:34,931 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 1 items
drwxr-xr-x   - hduser supergroup          0 2018-01-18 16:45 /datafiles
hduser@VBox:~$ hadoop fs -put /home/hduser/insertdata/hadoopins/*.txt  /datafiles
2018-01-18 16:50:23,004 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
hduser@VBox:~$ hadoop fs -ls /datafiles
2018-01-18 16:50:39,586 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 4 items
-rw-r--r--   1 hduser supergroup        255 2018-01-18 16:50 /datafiles/basketball_leagues.txt
-rw-r--r--   1 hduser supergroup     676895 2018-01-18 16:50 /datafiles/dimPlayers.txt
-rw-r--r--   1 hduser supergroup     371215 2018-01-18 16:50 /datafiles/dimteams.txt
-rw-r--r--   1 hduser supergroup    2046896 2018-01-18 16:50 /datafiles/factFinal.txt
hduser@VBox:~$ cd /home/spyros/spark-2.2.1-bin-hadoop2.7/bin


================================================================================================================================
================================================================================================================================
log tou spark

hduser@VBox:/home/spyros/spark-2.2.1-bin-hadoop2.7/bin$ ./spark-shell
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
18/01/18 23:39:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/01/18 23:39:25 WARN Utils: Your hostname, VBox resolves to a loopback address: 127.0.1.1; using 192.168.1.67 instead (on interface enp0s3)
18/01/18 23:39:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Spark context Web UI available at http://192.168.1.67:4040
Spark context available as 'sc' (master = local[*], app id = local-1516311567710).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.2.1
      /_/
         
Using Scala version 2.11.8 (OpenJDK 64-Bit Server VM, Java 1.8.0_151)
Type in expressions to have them evaluated.
Type :help for more information.

scala> val rdd1 = sc.textFile("hdfs://localhost:9000/datafiles/basketball_leagues.txt")
rdd1: org.apache.spark.rdd.RDD[String] = hdfs://localhost:9000/datafiles/basketball_leagues.txt MapPartitionsRDD[1] at textFile at <console>:24

scala> val rdd2 = sc.textFile("hdfs://localhost:9000/datafiles/dimPlayers.txt")
rdd2: org.apache.spark.rdd.RDD[String] = hdfs://localhost:9000/datafiles/dimPlayers.txt MapPartitionsRDD[3] at textFile at <console>:24

scala> val rdd3 = sc.textFile("hdfs://localhost:9000/datafiles/dimteams.txt")
rdd3: org.apache.spark.rdd.RDD[String] = hdfs://localhost:9000/datafiles/dimteams.txt MapPartitionsRDD[5] at textFile at <console>:24

scala> val rdd4 = sc.textFile("hdfs://localhost:9000/datafiles/factFinal.txt")
rdd4: org.apache.spark.rdd.RDD[String] = hdfs://localhost:9000/datafiles/factFinal.txt MapPartitionsRDD[7] at textFile at <console>:24

scala> 

scala> val  FactFinal = rdd4.map(line => (((line.split(",")(0))),(line.split(",")(1)),(line.split(",")(2)),(line.split(",")(3)),(line.split(",")(4)),(line.split(",")(8).toInt)))
FactFinal: org.apache.spark.rdd.RDD[(String, String, String, String, String, Int)] = MapPartitionsRDD[8] at map at <console>:26

scala> 

scala> 

scala> val firstline = rdd1.first()
firstline: String = lgID,name

scala> val rdd12 =rdd1.filter(in => !in.equals(firstline))
rdd12: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[9] at filter at <console>:28

scala> val DimLeagues= rdd12.map(line => ((line.split(",")(0)),(line.split(",")(1))))
DimLeagues: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[10] at map at <console>:30

scala> 

scala> val DimPlayerstats = rdd2.map(line =>((line.split(",")(0)),(line.split(",")(2)),(line.split(",")(4)),(line.split(",")(9)),(line.split(",")(12)),(line.split(",")(16))))
DimPlayerstats: org.apache.spark.rdd.RDD[(String, String, String, String, String, String)] = MapPartitionsRDD[11] at map at <console>:26

scala> 

scala> val DimTeams = rdd3.map(line => ((line.split(",")(58)),(line.split(",")(0)),(line.split(",")(7)),(line.split(",")(51)),(line.split(",")(55))))
DimTeams: org.apache.spark.rdd.RDD[(String, String, String, String, String)] = MapPartitionsRDD[12] at map at <console>:26

scala> 

scala> val uniyear = rdd4.map(line => (line.split(",")(1)))
uniyear: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[13] at map at <console>:26

scala> val DimYears = uniyear.distinct
DimYears: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[16] at distinct at <console>:28

scala> 

scala> 

scala> 

scala> val remap1ff = FactFinal.map(a => ((a._4),(a._1,a._2,a._3,a._5,a._6)))
remap1ff: org.apache.spark.rdd.RDD[(String, (String, String, String, String, Int))] = MapPartitionsRDD[17] at map at <console>:28

scala> 

scala> val reduce1 = remap1ff.reduceByKey((a, b) =>(a._1,a._2,a._3,a._4,a._5+b._5)).map(a=>(a._1,(a._2._1,a._2._2,a._2._3,a._2._4,a._2._5)))
reduce1: org.apache.spark.rdd.RDD[(String, (String, String, String, String, Int))] = MapPartitionsRDD[19] at map at <console>:30

scala> 

scala> 

scala> 

scala> val protoerot = DimLeagues.join(reduce1).map(a=> (a._1,a._2._1,a._2._2._5))
protoerot: org.apache.spark.rdd.RDD[(String, String, Int)] = MapPartitionsRDD[23] at map at <console>:40

scala> 

scala> 

scala> val protoerot1 = protoerot.filter(x => x._1 == "NBA") 
protoerot1: org.apache.spark.rdd.RDD[(String, String, Int)] = MapPartitionsRDD[24] at filter at <console>:42

scala> protoerot1.saveAsTextFile("/home/hduser/NBA_YEARS")
                                                                                
scala> val remap2ff = FactFinal.map(a => ((a._2),(a._1,a._3,a._4,a._5,a._6)))
remap2ff: org.apache.spark.rdd.RDD[(String, (String, String, String, String, Int))] = MapPartitionsRDD[26] at map at <console>:28

scala> val reduce2 = remap2ff.reduceByKey((a, b) =>(a._1,a._2,a._3,a._4,a._5+b._5)).map(a=>(a._2._4,(a._1,a._2._1,a._2._2,a._2._3,a._2._5)))
reduce2: org.apache.spark.rdd.RDD[(String, (String, String, String, String, Int))] = MapPartitionsRDD[28] at map at <console>:30

scala> val remaptm = DimTeams.map(a=> ((a._1),(a._2,a._3,a._4,a._5)))
remaptm: org.apache.spark.rdd.RDD[(String, (String, String, String, String))] = MapPartitionsRDD[29] at map at <console>:28

scala> 

scala> val defterojoin1 = remaptm.join(reduce2).map(a=>(a._2._2._1,a._1,a._2._1._2,a._2._2._5))
defterojoin1: org.apache.spark.rdd.RDD[(String, String, String, Int)] = MapPartitionsRDD[33] at map at <console>:38

scala> val protofilt= defterojoin1.filter(x => (x._1 == "1991" || x._1 == "1992" || x._1 == "1993"))
protofilt: org.apache.spark.rdd.RDD[(String, String, String, Int)] = MapPartitionsRDD[34] at filter at <console>:40

scala> val defterofilt = protofilt.filter(x => x._3 == "Atlanta Hawks")
defterofilt: org.apache.spark.rdd.RDD[(String, String, String, Int)] = MapPartitionsRDD[35] at filter at <console>:42

scala> val tritoremap = defterofilt.map(x => (x._1,x._2,x._4))
tritoremap: org.apache.spark.rdd.RDD[(String, String, Int)] = MapPartitionsRDD[36] at map at <console>:44

scala> 

scala> tritoremap.saveAsTextFile("/home/hduser/AtlantaHawks_years91_92_93") 
                                                                                
scala> 

